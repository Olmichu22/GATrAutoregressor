# =========================================================
# Configuración de GATrAutoRegressor para Particle Flow
# =========================================================
#
# Este archivo contiene la configuración por defecto para el
# entrenamiento del modelo GATrAutoRegressor.
#
# Uso:
#   python train_gatr_autoregressor.py --config configs/gatr_autoregressor_default.yaml
#
# Los parámetros de línea de comandos tienen prioridad sobre
# los valores de este archivo.
# =========================================================

# -------------------------
# Seed para reproducibilidad
# -------------------------
seed: 42

# -------------------------
# Directorios
# -------------------------
checkpoint_dir: "checkpoints/gatr_autoregressor"
resume_from: null  # Ruta a un checkpoint para continuar entrenamiento

# -------------------------
# Configuración de datos
# -------------------------
data:
  # Lista de archivos .npz generados por convert_to_nn_format.py
  # dataset_paths:
  #   - "data/out_reco_edm4hep_edm4hep_1.npz"
  #   - "data/out_reco_edm4hep_edm4hep_2.npz"
  #   - "data/out_reco_edm4hep_edm4hep_3.npz"
  #   - "data/out_reco_edm4hep_edm4hep_4.npz"
  #   - "data/out_reco_edm4hep_edm4hep_5.npz"
  #   - "data/out_reco_edm4hep_edm4hep_6.npz"

    # - "data/train_events_2.npz"
  
  batch_size: 16
  num_workers: 4
  val_ratio: 0.1
  mode: "memory"    # "memory" (más rápido, más RAM) o "lazy" (menos RAM)
  pin_memory: true

# -------------------------
# Configuración del preprocesador
# -------------------------
preprocessor:
  # Normalización z-score de coordenadas espaciales
  normalize_coords: true
  
  # Normalización z-score de energía (después de log si aplica)
  normalize_energy: true
  normalize_momentum: true
  
  # Transformación logarítmica (recomendado para energía/momentum)
  log_energy: true
  log_momentum: true
  log_eps: 1.0e-6
  
  # Normalización de momentum de PFOs (generalmente no necesario)
  normalize_pfo_momentum: false
  log_pfo_energy: true

# -------------------------
# Configuración del modelo
# -------------------------
model:
  # Modo del modelo
  # - "whole_detector": procesa todos los hits juntos
  # - "detector_split": procesa cada subdetector por separado (4 módulos)
  mode: "whole_detector"
  
  # ---- Encoder GATr (para whole_detector) ----
  hidden_mv: 32        # Dimensión de multivectores
  hidden_s: 128         # Dimensión de escalares
  num_blocks: 3        # Número de bloques GATr
  in_s_channels: 6     # Canales de entrada escalares (del dataset)
  in_mv_channels: 1    # Canales de entrada de multivectores
  out_mv_channels: 1   # Canales de salida de multivectores
  dropout: 0.0         # Dropout
  out_s_channels: null # Si null, usa hidden_s
  
  # ---- Encoder GATr por detector (para detector_split) ----
  # Listas de 4 elementos: [INNER_TRACKER, ECAL, HCAL, MUON_TRACKER]
  split_hidden_mv: [32, 32, 32, 32]
  split_hidden_s: [64, 64, 64, 64]
  split_num_blocks: [3, 3, 3, 3]
  split_in_s_channels: [6, 6, 6, 6]
  split_dropout: [0.1, 0.1, 0.1, 0.1]
  
  # ---- Módulo autoregresivo ----
  ar_hidden_mv: 32
  ar_hidden_s: 128
  ar_num_blocks: 5
  ar_out_mv_channels: 1
  ar_out_s_channels: null
  ar_dropout: 0.0
  max_steps: 128
  max_ar_steps_train: null  # e.g. 16/32 para depurar OOM en training

  # Assingment head
  use_gatr_assignment_head: true
  assignment_head_num_blocks: 2
  assignment_head_hidden_s: 64
  assignment_head_dropout: 0.0

# -------------------------
# Configuración de entrenamiento
# -------------------------
trainer:
  # ---- Optimización ----
  devices: [0]  # Lista de IDs de GPU (e.g., [0, 1]) o "auto"
  strategy: "ddp"  # "auto", "ddp", "ddp_spawn", etc.
  lr: 1.0e-4
  weight_decay: 1.0e-4
  
  # Scheduler: "step" o "cosine"
  scheduler: "cosine"
  warmup_pct: 0.02     # Porcentaje de épocas para warm-up lineal de LR (0.1 = 10%)
  warmup_start_factor: 0.05  # LR inicial del warm-up como fracción de lr
  decay_steps: 30      # Para scheduler "step"
  decay_rate: 0.8      # Para scheduler "step"
  lr_min: 1.0e-6       # Para scheduler "cosine"
  
  # ---- Training loop ----
  max_epochs: 1000
  gradient_clip_val: 2.0 # 2
  gradient_clip_algorithm: "norm"
  precision: "bf16-mixed"  # "32", "16-mixed", "bf16-mixed"
  accumulate_grad_batches: 1
  
  # ---- Pesos de loss ----
  lambda_dir: 1.0      # Dirección del momentum
  lambda_mag: 1.0      # Magnitud del momentum
  lambda_pid: 1.0      # Clasificación de PID
  lambda_charge: 0.5   # Predicción de carga
  lambda_assign: 2.0   # Asignación hit→PFO
  lambda_stop: 0.5     # Predicción de stop
  
  # ---- Callbacks ----
  early_stopping_patience: 30  # 0 para desactivar
  early_stopping_metric: "val/loss"
  save_top_k: 3
  
  # ---- Logging ----
  log_every_n_steps: 32
  plot_every_n_steps: 5000
  num_sanity_val_steps: 2
  debug_memory: false
  debug_memory_interval: 5
  max_ar_steps_train: null  # límite opcional de pasos AR en training

# -------------------------
# Configuración de Weights & Biases
# -------------------------
wandb:
  project: "gatr-autoregressor"
  run_name: "bigmodel-v1"
  log_model: false
  offline: false  # true para desarrollo sin conexión
